// Copyright 2023 ONDEWO GmbH
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.https://ondewo.slack.com/archives/CAWPP61NY

syntax = "proto3";

package ondewo.s2t;
import "google/protobuf/empty.proto";
import "google/protobuf/struct.proto";

// Speech-to-text service
service Speech2Text {

    // Transcribes an audio file
    rpc TranscribeFile (TranscribeFileRequest) returns (TranscribeFileResponse) {};

    // Transcribes an audio stream.
    rpc TranscribeStream (stream TranscribeStreamRequest) returns (stream TranscribeStreamResponse) {};

    // Gets a speech to text pipeline corresponding to the id specified in S2tPipelineId. If no corresponding id is
    // found, raises ModuleNotFoundError in server.
    rpc GetS2tPipeline (S2tPipelineId) returns (Speech2TextConfig) {};

    // Creates a new speech to text pipeline from a Speech2TextConfig and registers the new pipeline in the server.
    rpc CreateS2tPipeline (Speech2TextConfig) returns (S2tPipelineId) {};

    // Deletes a pipeline corresponding to the id parsed in S2TPipelineId. If no corresponding id is
    // found, raises ModuleNotFoundError in server.
    rpc DeleteS2tPipeline (S2tPipelineId) returns (google.protobuf.Empty) {};

    // Updates a pipeline with the id specified in Speech2TextConfig with the new config. If no corresponding id is
    // found, raises ModuleNotFoundError in server.
    rpc UpdateS2tPipeline (Speech2TextConfig) returns (google.protobuf.Empty) {};

    // Lists all speech to text pipelines.
    rpc ListS2tPipelines (ListS2tPipelinesRequest) returns (ListS2tPipelinesResponse) {};

    // Returns a message containing a list of all languages for which there exist pipelines.
    rpc ListS2tLanguages (ListS2tLanguagesRequest) returns (ListS2tLanguagesResponse) {};

    // Returns a message containing a list of all domains for which there exist pipelines.
    rpc ListS2tDomains (ListS2tDomainsRequest) returns (ListS2tDomainsResponse) {};

    // Returns a message containing the version of the running speech to text server.
    rpc GetServiceInfo (google.protobuf.Empty) returns (S2TGetServiceInfoResponse) {};

    // Given a list of pipeline ids, returns a list of LanguageModelPipelineId messages containing the pipeline
    // id and a list of the language models loaded in the pipeline.
    rpc ListS2tLanguageModels (ListS2tLanguageModelsRequest) returns (ListS2tLanguageModelsResponse) {};

    // Create a user language model.
    rpc CreateUserLanguageModel (CreateUserLanguageModelRequest) returns (google.protobuf.Empty) {};

    // Delete a user language model.
    rpc DeleteUserLanguageModel (DeleteUserLanguageModelRequest) returns (google.protobuf.Empty) {};

    // Add data to a user language model.
    rpc AddDataToUserLanguageModel (AddDataToUserLanguageModelRequest) returns (google.protobuf.Empty) {};

    // Train a user language model.
    rpc TrainUserLanguageModel (TrainUserLanguageModelRequest) returns (google.protobuf.Empty) {};

}

///////////////////////////
//         Enums         //
///////////////////////////

// The decoding configuration
enum Decoding {

    // decoding will be defined by the pipeline config
    DEFAULT = 0;

    // greedy decoding will be used independently on pipeline config
    GREEDY = 1;

    // beam search will be used independently on pipeline config
    BEAM_SEARCH_WITH_LM = 2;

    // beam search without LM head, to configure decoding mode for seq2seq models.
    BEAM_SEARCH = 3;

}

enum InferenceBackend {

    // Not set
    INFERENCE_BACKEND_UNKNOWN = 0;

    // Run pytorch model
    INFERENCE_BACKEND_PYTORCH = 1;

    // Run flax model
    INFERENCE_BACKEND_FLAX = 2;

}

///////////////////////////
// Configuration Message //
///////////////////////////

// Configuration for a request to transcribe audio
message TranscribeRequestConfig {

    // Required. id of the pipeline (model setup) that will generate audio
    string s2t_pipeline_id = 1;

    // Optional. decoding type
    Decoding decoding = 2;

    // Optional. Language model name for decoding (in none, default will be taken from config file)
    oneof oneof_language_model_name {

        // Name of the language model
        string language_model_name = 3;
    }

    // Optional. Configuration of spelling correction and normalization
    oneof oneof_post_processing {

        // The postprocessing options
        PostProcessingOptions post_processing = 4;
    }

    // Optional. Configuration for the detection of utterances
    oneof oneof_utterance_detection {

        // The utterance detection options
        UtteranceDetectionOptions utterance_detection = 5;
    }

    // Optional. Which method to be used for voice activity detection
    oneof voice_activity_detection {

        // Voice activity detection with pyannote
        Pyannote pyannote = 6;
    }

    // Optional. Specify which data to return
    oneof oneof_return_options {

        // The transcribe return options
        TranscriptionReturnOptions return_options = 8;
    }

    // Optional. Specify language of transcription to return
    optional string language = 9;

    // Optional. Specify task of s2t model, e.g. 'transcribe' and 'translate'
    optional string task = 10;

    // Optional. s2t_service_config provides the configuration of the service such as API key, bearer tokens, JWT,
    // and other header information as key value pairs, e.g., <pre><code>MY_API_KEY='LKJDIFe244LKJOI'</code></pre>
    optional google.protobuf.Struct s2t_service_config = 11;

    // Optional. Defines the cloud provider's specific configuration for using speech to text cloud services
    // The default value is None.
    optional S2tCloudProviderConfig s2t_cloud_provider_config = 12;

}

// Configuration for cloud provider settings for Speech-to-Text (S2T).
message S2tCloudProviderConfig {

    // Optional. Configuration for Amazon web service speech-to-text provider.
    optional S2tCloudProviderConfigAmazon s2t_cloud_provider_config_amazon = 1;

    // Optional. Configuration for DeepGram speech-to-text provider.
    optional S2tCloudProviderConfigDeepgram s2t_cloud_provider_config_deepgram = 2;

    // Optional. Configuration for Google speech-to-text provider.
    optional S2tCloudProviderConfigGoogle s2t_cloud_provider_config_google = 3;

    // Optional. Configuration for Microsoft Azure speech-to-text provider.
    optional S2tCloudProviderConfigMicrosoft s2t_cloud_provider_config_microsoft = 4;

}

// Configuration details specific to the Amazon web service speech-to-text provider.
message S2tCloudProviderConfigAmazon {

    // Optional. Enables or disables partial_results_stabilization feature. More details at:
    // https://docs.aws.amazon.com/transcribe/latest/dg/streaming-partial-results.html#streaming-partial-result-stabilization
    optional bool enable_partial_results_stabilization = 1;

    // Optional. You can use this field to set the stability level of the transcription results.
    // A higher stability level means that the transcription results are less likely to change.
    // Higher stability levels can come with lower overall transcription accuracy.
    // Defaults to "high" if not set explicitly.
    optional string partial_results_stability = 2;

    // Optional. The name of your customize language model you want to use.
    // More details at: https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html
    optional string language_model_name = 3;

    // Optional. The name of your customize language model you want to use.
    // More details at: https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html
    optional string vocabulary_name = 4;

}

// Configuration details specific to the Deepgram speech-to-text provider.
message S2tCloudProviderConfigDeepgram {

    // Optional. Enables or disables punctuate feature of Deepgram to add punctuations to the resulted transcript.
    // More details at: https://developers.deepgram.com/docs/punctuation
    optional bool punctuate = 1;

    // Optional. Enables or disables smart_format feature of Deepgram transcription result to improve readability.
    // More details at: https://developers.deepgram.com/docs/smart-format
    optional bool smart_format = 2;

    // Optional. Enables or disables numerals feature of Deepgram to convert numbers to numeric form in the resulted
    // transcript. More details at: https://developers.deepgram.com/docs/numerals
    optional bool numerals = 3;

    // Optional. Enables or disables measurements feature of Deepgram to convert measurement units (i.e. Kilogram)
    // to abbreviated form (i.e. Kg) in the resulted transcript.
    // More details at: https://developers.deepgram.com/docs/measurements
    optional bool measurements = 4;

    // Optional. Enables or disables dictation feature of Deepgram to convert spoken dictation commands into their
    // corresponding punctuation marks. More details at: https://developers.deepgram.com/docs/dictation
    optional bool dictation = 5;

}

// Configuration details specific to the Google speech-to-text provider.
message S2tCloudProviderConfigGoogle {

    // Optional. Enables or disables automatic_punctuation feature of Google s2t to add punctuations to the resulted
    // transcript. More details at: https://cloud.google.com/speech-to-text/docs/automatic-punctuation
    optional bool enable_automatic_punctuation = 1;

    // Optional. Enables or disables word_time_offsets feature of Google s2t to add word-level timestamps (time-offset)
    // to the resulted transcript. More details at: https://cloud.google.com/speech-to-text/docs/async-time-offsets
    optional bool enable_word_time_offsets = 2;

    // Optional. Enables or disables word_confidence feature of Google s2t to add word-level confidence scores
    // to the resulted transcript. More details at: https://cloud.google.com/speech-to-text/docs/word-confidence
    optional bool enable_word_confidence = 3;

    // Optional. Enables or disables transcript_normalization feature of Google s2t to automatically
    // replace parts of the transcript with phrases of your choosing. More details at:
    // https://cloud.google.com/speech-to-text/v2/docs/reference/rpc/google.cloud.speech.v2#transcriptnormalization
    optional bool transcript_normalization = 4;

    // Optional. Maximum number of recognition hypotheses to be returned, may be returned fewer than max_alternatives.
    // Valid values are 0-30. A value of 0 or 1 will return a maximum of one. If omitted, will return a maximum of one.
    optional int32 max_alternatives = 5;

}

// Configuration details specific to the Microsoft Azure speech-to-text provider.
message S2tCloudProviderConfigMicrosoft {

    // Optional. Enables or disables the Microsoft Azure fast transcription API. It is faster than SDK but is in
    // preview version.
    // More details at: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/fast-transcription-create
    optional bool use_fast_transcription_api = 1;

    // Optional. Enables or disables the `detailed` format for the result of Microsoft Azure s2t service
    // to add timestamps and confidences to the resulted transcript.
    optional bool use_detailed_output_format = 2;

}

// Configuration of the return values of a transcribe request
message TranscriptionReturnOptions {

    // should server make response indicating that the beginning of the speech was detected
    bool return_start_of_speech = 1;

    // should s2t server return audio bytes of transcribed utterance
    bool return_audio = 2;

    // Whether or not to return confidence scores
    bool return_confidence_score = 3;

    // Whether or not to return alternative results from beam-search
    bool return_alternative_transcriptions = 4;

    // Optional. Number of alternative transcriptions results from beam-search or greedy-search
    int32 return_alternative_transcriptions_nr = 5;

    // Whether or not to return alternative results from beam-search
    bool return_alternative_words = 6;

    // Optional. Number of alternative words to results
    int32 return_alternative_words_nr = 7;

    // Optional. Whether or not to return timestamps of start and end of the words. Only used in TranscribeFile.
    bool return_word_timing = 8;

}

// Configuration of the options to detect utterances
message UtteranceDetectionOptions {

    // Whether or not to transcribe unfinished utterances.
    oneof oneof_transcribe_not_final {

        // Return also immediate transcription results
        bool transcribe_not_final = 1;
    }

    // if time between audio chunks exceeds next_chunk_timeout, stream will be stopped
    float next_chunk_timeout = 2;

}

// Configuration of the post-processing options
message PostProcessingOptions {

    // Whether to use spelling correction
    bool spelling_correction = 1;

    // Whether to disable normalization
    bool normalize = 2;

    // Post-processing configuration specifying the active post-processors in the pipeline, as well as their individual
    // configuration. If not set, all values are replaced by the ones in current pipeline.
    PostProcessing config = 3;

}

///////////////////////////
//  TRANSCRIPTION TYPE  //
///////////////////////////

// The transcription message
message Transcription {

    // The transcribed text
    string transcription = 1;

    // The corresponding confidence score. The confidence estimate between 0.0 and 1.0. A higher number
    // indicates an estimated greater likelihood that the recognized words are correct.
    float confidence_score = 2;

    // List of the words of transcription with their confidence scores and probable alternatives
    repeated WordDetail words = 3;

    // List of alternative transcriptions, confidence scores, words timings and alternative words
    repeated TranscriptionAlternative alternatives = 4;

}

message TranscriptionAlternative{

    // The alternative transcribed text
    string transcript = 1;

    // The corresponding confidence score to the alternative transcript.
    float confidence = 2;

    // A list of word-specific information for each recognized word, including word timings, confidence score of
    // the word and alternative words.
    repeated WordDetail words = 3;

}

// WordDetail provides word-specific information for recognized words.
message WordDetail {

    // The start time of the spoken word relative to the beginning of the audio.
    // The accuracy of the time offset can vary, and this is an experimental feature.
    float start_time = 1;

    // The end time of the spoken word relative to the beginning of the audio.
    // The accuracy of the time offset can vary, and this is an experimental feature.
    float end_time = 2;

    // The recognized word corresponding to this set of information.
    string word = 3;

    // The corresponding confidence score to the word.
    float confidence = 4;

    // List of alternative words and confidence scores of each.
    repeated WordAlternative word_alternatives = 5;

}

message WordAlternative{

    // The recognized word corresponding to this set of information.
    string word = 1;

    // The corresponding confidence score to the alternative word.
    float confidence = 2;

}

///////////////////////
// TRANSCRIBE STREAM //
///////////////////////

// Request to transcribe an audio stream
message TranscribeStreamRequest {

    // wav file to transcribe
    bytes audio_chunk = 1;

    // if it's the final chunk of the stream
    bool end_of_stream = 2;

    // The configuration to override the default configuration
    TranscribeRequestConfig config = 3;

    // Whether or not to mute the audio signal. Defaults to false.
    bool mute_audio = 4;

}

// The response message of a stream transcription
message TranscribeStreamResponse {

    // List of transcriptions with confidence level
    repeated Transcription transcriptions = 1;

    // The time the transcription took
    float time = 2;

    // Whether or not this transcription is final (transcribed texts might change if transcription is
    // started before the end of an utterance).
    bool final = 3;

    // is audio bytes of the utterance in response
    bool return_audio = 4;

    // audio bytes of the transcribed utterance
    bytes audio = 5;

    // is it a start of the utterance
    bool utterance_start = 6;

    // id of the transcribed audio file
    string audio_uuid = 7;

    // The configuration to override the default configuration
    oneof oneof_config {

        // The configuration for the transcription
        TranscribeRequestConfig config = 8;
    }

}

/////////////////////
// TRANSCRIBE FILE //
/////////////////////

// A request to transcribe an audio file
message TranscribeFileRequest {

    // wav file to transcribe
    bytes audio_file = 1;

    // The configuration to override the default configuration
    TranscribeRequestConfig config = 2;

}

// The response message for a transcribe file request
message TranscribeFileResponse {

    // List of transcriptions with confidence level
    repeated Transcription transcriptions = 1;

    // The time the transcription took
    float time = 2;

    // id of the transcribed audio file
    string audio_uuid = 3;

}

//////////////////////
// GET S2T PIPELINE //
//////////////////////

// The pipeline id for a specific pipeline configuration
message S2tPipelineId {

    // id of the model that will generate audio
    string id = 1;

}

////////////////////////
// LIST S2T PIPELINES //
////////////////////////

// Request to list all speech-to-text pipelines. Optionally also filter criteria can be set
message ListS2tPipelinesRequest {

    // Filter for languages
    repeated string languages = 1;

    // Filter for pipeline owners
    repeated string pipeline_owners = 2;

    // Filter for domains
    repeated string domains = 3;

    // If true, return only registered pipelines.
    // Default false: return registered and persisted (from config files) configs.
    bool registered_only = 4;

}

// ListS2tPipelinesResponse is used to return a list of all speech-to-text pipelines.
message ListS2tPipelinesResponse {

    // A list of Speech2TextConfig message instances containing the configuration of each pipeline.
    // Example: [{id: "pipeline_1", description: {language: "en"}, active: true, ...}, {id: "pipeline_2",
    // description: {language: "fr"}, active: true, ...}]
    repeated Speech2TextConfig pipeline_configs = 1;
}

////////////////////////
// LIST S2T LANGUAGES //
////////////////////////

// ListS2tLanguagesRequest is used to request a list of available languages. Optionally, filters can be set.
message ListS2tLanguagesRequest {

    // Filter for domains.  Example: ["medical", "finance"]
    repeated string domains = 1;

    // Filter for pipeline owners.
    // Example: ["ondewo", "partner_company"]
    repeated string pipeline_owners = 2;

}

// Response message to list available languages
message ListS2tLanguagesResponse {

    // available languages
    repeated string languages = 1;

}

//////////////////////
// LIST S2T DOMAINS //
//////////////////////

// Request message to list available domains. Optionally also filters can be set.
message ListS2tDomainsRequest {

    // Filter for languages
    repeated string languages = 1;

    // Filter for pipeline owner
    repeated string pipeline_owners = 2;

}

// Response message to list available domains
message ListS2tDomainsResponse {

    // domains available. Example: ["medical", "finance"]
    repeated string domains = 1;

}

//////////////////////
// GET SERVICE INFO //
//////////////////////

// S2TGetServiceInfoResponse is used to return version information about the speech-to-text service.
message S2TGetServiceInfoResponse {

    // Version number based on semantic versioning, e.g. "4.2.0".
    string version = 1;
}

///////////////////////////////////
// SPEECH-2-TEXT PIPELINE CONFIG //
///////////////////////////////////

// Speech2TextConfig is a configuration message for the speech-to-text pipeline
message Speech2TextConfig {

    // Unique identifier for the configuration.
    string id = 1;

    // Description of the speech-to-text system.
    S2TDescription description = 2;

    // Indicates if the configuration is active.
    bool active = 3;

    // Configuration for inference models.
    S2TInference inference = 4;

    // Configuration for the streaming server.
    StreamingServer streaming_server = 5;

    // Configuration for voice activity detection.
    VoiceActivityDetection voice_activity_detection = 6;

    // Configuration for post-processing.
    PostProcessing post_processing = 7;

    // Configuration for logging.
    Logging logging = 8;

}


// S2TDescription contains descriptive information about the speech-to-text pipeline.
message S2TDescription {

    // Language of the speech-to-text system.
    string language = 1;

    // Owner of the pipeline.
    string pipeline_owner = 2;

    // Domain of the speech-to-text system.
    string domain = 3;

    // Comments about the system.
    string comments = 4;

}


// S2TInference contains information about inference models used in the speech-to-text pipeline.
message S2TInference {

    // Configuration for the acoustic models.
    AcousticModels acoustic_models = 1;

    // Configuration for the language models.
    LanguageModels language_models = 2;

    // Configuration for the inference backend.
    InferenceBackend inference_backend = 3;

}

// AcousticModels contains information about different types of acoustic models.
message AcousticModels {

    // Type of the acoustic model.
    string type = 1;

    // Configuration for the Wav2Vec model.
    Wav2Vec wav2vec = 2;

    // Configuration for the Wav2Vec model using Triton.
    Wav2VecTriton wav2vec_triton = 3;

    // Configuration for the Whisper model.
    Whisper whisper = 4;

    // Configuration for the Whisper model using Triton.
    WhisperTriton whisper_triton = 5;

    // Amazon web service cloud service inference settings.
    S2tCloudServiceAmazon s2t_cloud_service_amazon = 6;

    // Deepgram cloud service inference settings.
    S2tCloudServiceDeepgram s2t_cloud_service_deepgram = 7;

    // Google cloud service inference settings.
    S2tCloudServiceGoogle s2t_cloud_service_google = 8;

    // Microsoft Azure cloud service inference settings.
    S2tCloudServiceMicrosoft s2t_cloud_service_microsoft = 9;

}

// S2tCloudServiceAmazon message contains settings for the Amazon web service Cloud service inference.
message S2tCloudServiceAmazon {

    // Language of the audio to transcribe by Amazon web service s2t cloud service. It should be 4-letter language code
    // (BCP-47) e.g. 'en-US' or 'de-DE'.
    string language = 1;

    // Specifies if streaming mode of Amazon web service speech to text is available for the selected language,
    // otherwise batch mode transcription is used. See the list of languages and available transcription modes at:
    // https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html
    bool streaming_available = 2;

    // Enables or disables partial_results_stabilization feature. More details at:
    // https://docs.aws.amazon.com/transcribe/latest/dg/streaming-partial-results.html#streaming-partial-result-stabilization
    bool enable_partial_results_stabilization = 3;

    // You can use this field to set the stability level of the transcription results.
    // A higher stability level means that the transcription results are less likely to change.
    // Higher stability levels can come with lower overall transcription accuracy.
    // Defaults to "high" if not set explicitly.
    string partial_results_stability = 4;

    // The name of your customize language model you want to use.
    // More details at: https://docs.aws.amazon.com/transcribe/latest/dg/custom-language-models.html
    string language_model_name = 5;

    // The name of your customize language model you want to use.
    // More details at: https://docs.aws.amazon.com/transcribe/latest/dg/custom-vocabulary.html
    string vocabulary_name = 6;

}

// S2tCloudServiceDeepgram message contains settings for the Deepgram Cloud service inference.
message S2tCloudServiceDeepgram {

    // Model name from one of the speech-to-text models provided by Deepgram for the desired use-case.
    // Provided model names and details at: https://developers.deepgram.com/docs/model
    string model_name = 1;

    // Language of the audio to transcribe by Deepgram s2t cloud service. It should be 4-letter language code
    // (BCP-47) e.g. 'en-US' or 'de-DE'.
    string language = 2;

    // Enables or disables punctuate feature of Deepgram to add punctuations to the resulted transcript.
    // More details at: https://developers.deepgram.com/docs/punctuation
    bool punctuate = 3;

    // Enables or disables smart_format feature of Deepgram transcription result to improve readability.
    // More details at: https://developers.deepgram.com/docs/smart-format
    bool smart_format = 4;

    // Enables or disables numerals feature of Deepgram to convert numbers to numeric form in the resulted transcript.
    // More details at: https://developers.deepgram.com/docs/numerals
    bool numerals = 5;

    // Enables or disables measurements feature of Deepgram to convert measurement units (i.e. Kilogram)
    // to abbreviated form (i.e. Kg) in the resulted transcript.
    // More details at: https://developers.deepgram.com/docs/measurements
    bool measurements = 6;

    // Enables or disables dictation feature of Deepgram to convert spoken dictation commands into their corresponding
    // punctuation marks. More details at: https://developers.deepgram.com/docs/dictation
    bool dictation = 7;

}

// S2tCloudServiceGoogle message contains settings for the Google Cloud service inference.
message S2tCloudServiceGoogle {

    // Model name from one of the speech-to-text models provided by Google for the desired use-case.
    // Provided model names and details at: https://cloud.google.com/speech-to-text/docs/transcription-model
    string model_name = 1;

    // Language of the audio to transcribe by Google s2t cloud service. It should be 4-letter language code
    // (BCP-47) e.g. 'en-US' or 'de-DE'.
    string language = 2;

    // Enables or disables automatic_punctuation feature of Google s2t to add punctuations to the resulted transcript.
    // More details at: https://cloud.google.com/speech-to-text/docs/automatic-punctuation
    bool enable_automatic_punctuation = 3;

    // Enables or disables word_time_offsets feature of Google s2t to add word-level timestamps (time-offsets)
    // to the resulted transcript. More details at: https://cloud.google.com/speech-to-text/docs/async-time-offsets
    bool enable_word_time_offsets = 4;

    // Enables or disables word_confidence feature of Google s2t to add word-level confidence scores
    // to the resulted transcript. More details at: https://cloud.google.com/speech-to-text/docs/word-confidence
    bool enable_word_confidence = 5;

    // Enables or disables transcript_normalization feature of Google s2t to automatically
    // replace parts of the transcript with phrases of your choosing. More details at:
    // https://cloud.google.com/speech-to-text/v2/docs/reference/rpc/google.cloud.speech.v2#transcriptnormalization
    bool transcript_normalization = 6;

    // Maximum number of recognition hypotheses to be returned. The server may return fewer than max_alternatives.
    // Valid values are 0-30. A value of 0 or 1 will return a maximum of one. If omitted, will return a maximum of one.
    int32 max_alternatives = 7;

}

// S2tCloudServiceMicrosoft message contains settings for the Microsoft Azure Cloud service inference.
message S2tCloudServiceMicrosoft {

    // Language of the audio to transcribe by Microsoft Azure s2t cloud service. It should be 4-letter language code
    // (BCP-47) e.g. 'en-US' or 'de-DE'.
    string language = 1;

    // Enables or disables the Microsoft Azure fast transcription API. It is faster than SDK but is in preview version.
    // More details at: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/fast-transcription-create
    bool use_fast_transcription_api = 2;

    // Enables or disables the `detailed` format for the result of Microsoft Azure s2t service
    // to add timestamps and confidences to the resulted transcript.
    bool use_detailed_output_format = 3;

}

// Whisper contains information about the Whisper model.
message Whisper {

    // Path to the model.
    string model_path = 1;

    // Indicates if GPU is used.
    bool use_gpu = 2;

    // Default language of the model.
    string language = 3;

    // Default task of the model.
    string task = 4;

}

// WhisperTriton contains information about the Whisper model using Triton.
message WhisperTriton {

    // Path to the processor.
    string processor_path = 1;

    // Name of the Triton model.
    string triton_model_name = 2;

    // Version of the Triton model.
    string triton_model_version = 3;

    // Timeout for checking model status.
    int64 check_status_timeout = 4;

    // Default language of the model.
    string language = 5;

    // Default task of the model. E.g., transcribe, translate, etc.
    string task = 6;

    // Host name of triton inference server that serves the WhisperTriton model
    string triton_server_host = 7;

    // Port number of triton inference server that serves the WhisperTriton model
    int64 triton_server_port = 8;

}

// Wav2Vec contains information about the Wav2Vec model.
message Wav2Vec {

    // Path to the model.
    string model_path = 1;

    // Indicates if GPU is used.
    bool use_gpu = 2;

}

// Wav2VecTriton contains information about the Wav2Vec model using Triton.
message Wav2VecTriton {

    // Path to the processor.
    string processor_path = 1;

    // Name of the Triton model.
    string triton_model_name = 2;

    // Version of the Triton model.
    string triton_model_version = 3;

    // Timeout for checking model status.
    int64 check_status_timeout = 4;

    // Host name of triton inference server that serves the Wav2VecTriton model
    string triton_server_host = 5;

    // Port number of triton inference server that serves the Wav2VecTriton model
    int64 triton_server_port = 6;
}

// PtFiles contains information about PT files.
message PtFiles {

    // Path to the PT files.
    string path = 1;

    // Step for the PT files.
    string step = 2;

}

// CkptFile contains information about checkpoint files.
message CkptFile {

    // Path to the checkpoint file.
    string path = 1;

}


// LanguageModels contains information about language models.
message LanguageModels {

    // Path to the directory of language models.
    string path = 1;

    // Beam size for the search algorithm.
    int64 beam_size = 2;

    // Default language model to be selected if none is given.
    string default_lm = 3;

    // Weight for the language model scorer (alpha).
    float beam_search_scorer_alpha = 4;

    // Weight for the word insertion penalty (beta).
    float beam_search_scorer_beta = 5;

}

// StreamingServer contains information about the streaming server.
message StreamingServer {

    // Hostname of the streaming server.
    string host = 1;

    // Port number of the streaming server.
    int64 port = 2;

    // Output style for the streaming server.
    string output_style = 3;

    // Configuration for streaming speech recognition.
    StreamingSpeechRecognition streaming_speech_recognition = 4;

}

// StreamingSpeechRecognition contains information about streaming speech recognition settings.
message StreamingSpeechRecognition {

    // Indicates whether to transcribe non-final results.
    bool transcribe_not_final = 1;

    // Decoding method for speech recognition.
    string decoding_method = 2;

    // Sampling rate for audio input.
    int64 sampling_rate = 3;

    // Minimum audio chunk size for processing.
    int64 min_audio_chunk_size = 4;

    // Timeout between audio chunks; if exceeded, the stream will be stopped.
    float next_chunk_timeout = 5;

}

// VoiceActivityDetection contains information about voice activity detection settings.
message VoiceActivityDetection {

    // Indicates if voice activity detection is active.
    string active = 1;

    // Sampling rate for voice activity detection.
    int64 sampling_rate = 2;

    // Configuration for the Pyannote model.
    Pyannote pyannote = 3;

}

// Pyannote contains configuration for the Pyannote voice activity detection model.
// Library: [pyannote-audio](https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/voice_activity_detection.ipynb)
message Pyannote {

    // Full name of the Pyannote model.
    string model_name = 1;

    // Minimum audio size for processing.
    int64 min_audio_size = 2;

    // Fill inactive regions shorter than that many seconds.
    // Example [notebook](https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/voice_activity_detection.ipynb)
    float min_duration_off = 3;

    // Remove active regions shorter than that many seconds
    // Example [notebook](https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/voice_activity_detection.ipynb)
    float min_duration_on = 4;

    // Host name of triton inference server that serves the Pyannote model
    string triton_server_host = 5;

    // Port number of triton inference server that serves the Pyannote model
    int64 triton_server_port = 6;

}

// PostProcessing contains the configuration for post-processing.
message PostProcessing {

    // List of names of active post-processors.
    repeated string pipeline = 1;

    // Post-processor configurations.
    PostProcessors post_processors = 2;

}

// PostProcessors contains configurations for post-processors.
message PostProcessors {

    // Configuration of the SymSpell spelling correction.
    SymSpell sym_spell = 1;

    // Configuration of the normalization object.
    S2TNormalization normalization = 2;

}

// SymSpell contains configuration for the SymSpell spelling correction.
message SymSpell {

    // Path to the dictionary used by symspell
    string dict_path = 1;

    // The maximal edit-distance to consider for spelling correction (affects performance - bigger number
    // takes longer to process!)
    int64 max_dictionary_edit_distance = 2;

    // The length (number of characters) of the prefix to consider for filtering
    int64 prefix_length = 3;

}

// S2TNormalization contains configuration for the speech-to-text normalization.
message S2TNormalization {

    // Language for normalization of transcriptions.
    string language = 1;

    // List of names of active normalizations.
    repeated string pipeline = 2;

}

// Logging contains configuration for logging.
message Logging {

    // Type of logging.
    string type = 1;

    // Path for logging.
    string path = 2;

}

///////////////////////////////////////////
// GET LIST OF AVAILABLE LANGUAGE MODELS //
///////////////////////////////////////////

// ListS2tLanguageModelsRequest is used to request a list of available language models for specified pipelines.
message ListS2tLanguageModelsRequest {

    // List of pipeline IDs to retrieve their available language models.
    // Example: ["pipeline_1", "pipeline_2"]
    repeated string ids = 1;

}

// LanguageModelPipelineId contains information about a pipeline and its available language models.
message LanguageModelPipelineId {

    // A pipeline ID. Example: "pipeline_1"
    string pipeline_id = 1;

    // A list of all available language models for the corresponding pipeline ID. Example: ["model_1", "model_2"]
    repeated string model_names = 2;

}

// ListS2tLanguageModelsResponse is used to return the available language models for specified pipelines.
message ListS2tLanguageModelsResponse {

    // Response is a list of LanguageModelPipelineId, where each element contains a pipeline ID and its associated
    // language models.
    // Example: [{pipeline_id: "pipeline_1", model_names: ["model_1", "model_2"]}, {pipeline_id: "pipeline_2",
    // model_names: ["model_3"]}]
    repeated LanguageModelPipelineId lm_pipeline_ids = 1;

}

/////////////////////////////////////
// CUSTOM LANGUAGE MODEL FEATURES  //
/////////////////////////////////////

// CreateUserLanguageModelRequest is used to request the creation of a new user-specific language model.
message CreateUserLanguageModelRequest {

    // Name of the language model to create. Example: "user_lm_1"
    string language_model_name = 1;

}

// DeleteUserLanguageModelRequest is used to request the deletion of a user-specific language model.
message DeleteUserLanguageModelRequest {

    // Name of the language model to delete. Example: "user_lm_1"
    string language_model_name = 1;

}

// AddDataToUserLanguageModelRequest is used to request the addition of data to a user-specific language model.
message AddDataToUserLanguageModelRequest {

    // Name of the language model to which to add data. Example: "user_lm_1"
    string language_model_name = 1;

    // Zip file containing data in the form of text files.
    // Example: A zip file with text files containing sentences or phrases in the target language.
    bytes zipped_data = 2;

}

// TrainUserLanguageModelRequest is used to request the training of a user-specific language model.
message TrainUserLanguageModelRequest {

    // Name of the language model to train. Example: "user_lm_1"
    string language_model_name = 1;

    // Order n of the ngram. Example: 3 (for trigram model)
    int64 order = 2;

}
